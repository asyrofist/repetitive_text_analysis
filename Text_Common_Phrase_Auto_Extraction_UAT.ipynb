{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory: Extract multiple phrases surrounding keywords into a dataset. Then, anonymize the dataset extractions by simply replacing the rex expression with nothing. After that, apply positive-negative sentiment analysis to the phrase to remove any potential connotations invoked by the initial phrasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting phrases rather than just sentential analysis allows for patterns of speech to be extracted from speeches and then compared for frequent useage.\n",
    "\n",
    "## It also helps humans build a phrase anonymizing map to go beyond just single words or a priori judgments about what is in speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict, OrderedDict\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jameslittiebrant/one_off_python_stuff/speech_comparability/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton = pd.read_csv('/Users/jameslittiebrant/one_off_python_stuff/speech_comparability/data/Clinton_Speeches_raw.csv')\n",
    "trump = pd.read_csv('/Users/jameslittiebrant/one_off_python_stuff/speech_comparability/data/Trump_Speeches_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton['month'] = clinton['month'].apply(lambda x: \"{:02d}\".format(x))\n",
    "clinton['date'] = clinton['date'].apply(lambda x: \"{:02d}\".format(x))\n",
    "clinton['year'] = clinton['year'].astype(str)\n",
    "\n",
    "trump['month'] = trump['month'].apply(lambda x: \"{:02d}\".format(x))\n",
    "trump['date'] = trump['date'].apply(lambda x: \"{:02d}\".format(x))\n",
    "trump['year'] = trump['year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rex_phrase_creator(word,before_start,before_stop,after_start,after_stop):\n",
    "    before_rex = []\n",
    "    after_rex = []\n",
    "    before_words = '\\S* '\n",
    "    after_words = ' \\S*'\n",
    "    word_rex = '\\s*{}\\s*'.format(word)\n",
    "    phrases = []\n",
    "    \n",
    "    for i in range(before_start,before_stop):\n",
    "        temp_b = before_words*i\n",
    "        temp_b = temp_b\n",
    "        before_rex.append(('b_{}'.format(i),temp_b))\n",
    "    \n",
    "    for i in range(after_start,after_stop):\n",
    "        temp_a = after_words*i\n",
    "        temp_a = temp_a\n",
    "        after_rex.append(('a_{}'.format(i),temp_a))\n",
    "    \n",
    "    for b_col,b_rex in before_rex:\n",
    "        for a_col,a_rex in after_rex:\n",
    "            total_phrase = '({}{}{})'.format(b_rex,word,a_rex)\n",
    "            column_name = word + '_' + b_col + '_' +  a_col\n",
    "            phrases.append((column_name,total_phrase))\n",
    "\n",
    "    for a_col,a_rex in after_rex:\n",
    "        total_phrase = '({}{})'.format(word,a_rex)\n",
    "        phrases.append((word + '_' + a_col,total_phrase))\n",
    "\n",
    "    for b_col,b_rex in before_rex:\n",
    "        total_phrase = '({}{})'.format(b_rex,word)\n",
    "        phrases.append((word + '_' + b_col,total_phrase))\n",
    "        \n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_for_processing(text_str):\n",
    "    clean_txt = text_str.replace('\\n','').replace('\\\\','').replace(',','')\\\n",
    "                    .replace('!','.').replace('?','.')\\\n",
    "                    .rstrip(' ').lstrip('')\\\n",
    "                    .split('.')\n",
    "    \n",
    "    clean_txt = [x.lstrip(' ').rstrip(' ').lower() for x in clean_txt if x.lstrip(' ').rstrip(' ') != '']\n",
    "    return clean_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_to_dataframe(txt,source_id,speaker):\n",
    "    df = pd.DataFrame(data=txt,columns=['sentence'])\n",
    "    df['source_id'] = source_id\n",
    "    df['speaker'] = speaker\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rex_searches(rex_args):\n",
    "    rexes = []\n",
    "    for search in rex_args:\n",
    "        searches = rex_phrase_creator(*search)\n",
    "        rexes.extend(searches)\n",
    "    return rexes\n",
    "\n",
    "def apply_rex_to_df(df,rexes):\n",
    "    apply_df = df.copy()\n",
    "    for col, rex in rexes:\n",
    "        apply_df[col] = apply_df['sentence'].str.lower().str.extract(rex)\n",
    "    return apply_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building automated functions for duplicate phrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_searches = []\n",
    "def key_to_columns(df,column_searches):\n",
    "    rexes = []\n",
    "    for column_key in column_searches:\n",
    "        search_rex = re.compile(r'{}'.format(column_key))\n",
    "        rexes.append(search_rex)\n",
    "    \n",
    "    column_selection = []\n",
    "    for key, rex in zip(column_searches,rexes):\n",
    "        cols = [col for col in df.columns if rex.search(col)]\n",
    "        column_selection.append((key,cols))\n",
    "        \n",
    "    return column_selection\n",
    "\n",
    "def key_cols_to_count(df,column_selection):\n",
    "    local_df = df.copy()\n",
    "    count_columns = []\n",
    "    for key, cols in column_selection:\n",
    "        count_col_name = key + '_count'\n",
    "        mention_name = key + '_mentioned'\n",
    "        if count_col_name in df.columns:\n",
    "            df = df.drop(count_col_name,axis=1)\n",
    "        if mention_name in df.columns:\n",
    "            df = df.drop(mention_name,axis=1)\n",
    "            \n",
    "        count_columns.append((key,cols,count_col_name))\n",
    "        local_df[count_col_name] = local_df[cols].notnull().sum(axis=1)\n",
    "        local_df[mention_name] = local_df[count_col_name] > 0\n",
    "\n",
    "    return local_df, count_columns\n",
    "\n",
    "# Groups duplicates by the speaker and then returns all the indexes for duplicates for easy comparison\n",
    "def duplicate_splitting(df,selection_columns,count_col):\n",
    "    local_df = df.copy()\n",
    "    duplicated_phrases = defaultdict(set)\n",
    "    for speaker,group_df in local_df[['speaker'] + selection_columns].groupby('speaker'):\n",
    "        group_df = group_df[group_df[count_col] > 0]\n",
    "        for col in selection_columns:\n",
    "            duplicated_series = local_df[col].dropna()\n",
    "            duplicated_series = duplicated_series.duplicated(keep=False)\n",
    "            duplicated_series = duplicated_series.loc[duplicated_series]\n",
    "            duplicated_phrases[col].update(list(duplicated_series.index))\n",
    "    return duplicated_phrases\n",
    "\n",
    "def split_duplicates_by_column(df,duplicated_phrases,selection_columns):\n",
    "    local_df = df.copy()\n",
    "    duplicated_splits = dict()\n",
    "    master_dupes = pd.DataFrame(columns=['sentence','speaker','dupe_phrases'])\n",
    "    for col in selection_columns:\n",
    "        duplicated_data = local_df.loc[duplicated_phrases[col],['sentence','speaker',col]]\\\n",
    "                                        .sort_values(by=col)\n",
    "        duplicated_splits[col] = duplicated_data\n",
    "        master_data = duplicated_data.copy()\n",
    "        master_data.columns = ['sentence','speaker','dupe_phrases']\n",
    "        master_dupes = master_dupes.append(master_data)\n",
    "        \n",
    "    return duplicated_splits, master_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_anonymizer(df,drop_columns=False,fill_back_columns=False):\n",
    "    if drop_columns:\n",
    "        local_df = df.drop(drop_columns,axis=1).copy()\n",
    "    else:\n",
    "        local_df = df.copy()\n",
    "        \n",
    "    token_search = re.compile(r'([a-zA-Z ]*)_')\n",
    "    \n",
    "    # below expression finds only the extraction columns, no other types\n",
    "    token_col_pairs = [(col,token_search.search(col).group(1)) for col in local_df.columns]\n",
    "    for col, token_removal in token_col_pairs:\n",
    "        local_df[col] = local_df[col].fillna('').str.replace(r'( ?{} ?)'.format(token_removal), ' ').replace('',np.nan)\n",
    "    \n",
    "    if fill_back_columns:\n",
    "        local_df = pd.concat([df[fill_back_columns],local_df],axis=1)\n",
    "    \n",
    "    return local_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = pd.DataFrame(columns=['sentence','source_id','speaker'])\n",
    "for index, row in clinton.iterrows():\n",
    "    source_id = row['year'] + row['month'] + row['date']\n",
    "    source_txt = row['text']\n",
    "    cleaned_txt = clean_data_for_processing(source_txt)\n",
    "    cleaned_df = split_text_to_dataframe(cleaned_txt,source_id,'clinton')\n",
    "    sentence_df = sentence_df.append(cleaned_df)\n",
    "\n",
    "for index, row in trump.iterrows():\n",
    "    source_id = row['year'] + row['month'] + row['date']\n",
    "    source_txt = row['text']\n",
    "    cleaned_txt = clean_data_for_processing(source_txt)\n",
    "    cleaned_df = split_text_to_dataframe(cleaned_txt,source_id,'trump')\n",
    "    sentence_df = sentence_df.append(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = sentence_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df.columns = ['sentence_location','sentence','source_id','speaker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches = [('god',1,4,1,4),\n",
    "                    ('country',3,6,3,6),('our country',3,5,3,5),\n",
    "                    ('america',3,6,3,6),('our america',3,5,3,5),\n",
    "                   ('illegal',1,5,1,5),('illegal alien',2,5,2,5), \n",
    "                      ('illegal immigrant',2,5,2,5),('immigrant',1,5,1,5),\n",
    "           ('democracy',1,4,1,4),('huge',1,3,1,3),('great',1,3,1,3),\n",
    "           ('amazing',1,3,1,3),('great',1,3,1,3),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the \"searches\" to create multiple extraction phrases and then apply to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 351 ms, total: 1min 20s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "search_rexes = create_rex_searches(searches)\n",
    "master_df = apply_rex_to_df(sentence_df,search_rexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keys_cols_to_count extracts all the auto-generated regex columns based on the column_sel variable. This can mirror the searches or be a selected subset of them to investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sel = key_to_columns(master_df,['immigrant','god','america','democracy','huge','great','amazing','great'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With the columns now found: key_cols_to_count counts the number of times that the regular expression matched in the spoken sentence, and also creates a _mentioned boolean columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df,count_cols = key_cols_to_count(master_df,column_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# duplicate returns all the indexes of where the specific regex phrase is found in the input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dupe_selections = dict()\n",
    "master_cols = dict()\n",
    "for capture_col, selection_cols, count_col in count_cols:\n",
    "    if count_col not in selection_cols:\n",
    "        selection_cols.append(count_col)\n",
    "    return_data = duplicate_splitting(master_df,selection_cols,count_col)\n",
    "    dupe_selections[capture_col] = return_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dupe_selections['america']['america_b_3_a_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split_duplicates_by_column returns a dictionary of the different duplicate dataframes and master_dupes which is a single dataframe of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupe_splits_dict = dict()\n",
    "master_dupes_dict = dict()\n",
    "for key,selection_columns,count_col in count_cols:\n",
    "    selections = [col for col in selection_columns if '_count' not in col]\n",
    "    dupe_splits,master_dupes = split_duplicates_by_column(master_df,dupe_selections[key],selections)\n",
    "    dupe_splits_dict[key] = dupe_splits\n",
    "    master_dupes_dict[key] = master_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_huge = master_dupes_dict['huge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigrant_deported = master_dupes_dict['immigrant'][master_dupes_dict['immigrant']['dupe_phrases']\\\n",
    "                                                  .str.contains('deported')]\n",
    "\n",
    "immigrant_gang = master_dupes_dict['immigrant'][master_dupes_dict['immigrant']['dupe_phrases']\\\n",
    "                                                  .str.contains('gang')]\n",
    "\n",
    "immigrant_murder = master_dupes_dict['immigrant'][master_dupes_dict['immigrant']['dupe_phrases']\\\n",
    "                                                  .str.contains('murdered')]\n",
    "\n",
    "immigrant_criminal = master_dupes_dict['immigrant'][master_dupes_dict['immigrant']['dupe_phrases']\\\n",
    "                                                  .str.contains('criminal')]\n",
    "\n",
    "immigrant_illegal = master_dupes_dict['immigrant'][master_dupes_dict['immigrant']['dupe_phrases']\\\n",
    "                                                  .str.contains('illegal')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocking_words = ['was','is','an','by']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_phrase_with_count(df,blocking_words):\n",
    "    phrase_counted_df = pd.DataFrame()\n",
    "    for sentence, temp_df in df.groupby('sentence'):\n",
    "        temp_df.reset_index(inplace=True)\n",
    "        temp_df = temp_df.drop_duplicates(subset=['dupe_phrases'])\n",
    "        first_words = temp_df['dupe_phrases'].str.extract(r'(\\w+) ')\n",
    "        first_words = first_words.isin(blocking_words)\n",
    "        temp_df = temp_df.loc[~first_words.values.flatten()]\n",
    "        temp_df['phrase_length'] = temp_df['dupe_phrases'].str.count(' ')\n",
    "        phrase_counted_df = phrase_counted_df.append(temp_df)\n",
    "        \n",
    "    phrase_counted_df = phrase_counted_df.drop_duplicates(subset=['dupe_phrases'])\n",
    "    phrase_counted_df = phrase_counted_df[phrase_counted_df['phrase_length'] > 1]\n",
    "\n",
    "    splitting_phrases = phrase_counted_df.sort_values(by='phrase_length',ascending=False)['dupe_phrases'].values\n",
    "\n",
    "    splitting_phrases = [x.strip() for x in splitting_phrases]\n",
    "\n",
    "    return phrase_counted_df.sort_values(by='phrase_length',ascending=False), splitting_phrases\n",
    "\n",
    "def split_sentences_on_common_phrases(org_df,splitting_phrases):\n",
    "    df = org_df.copy()\n",
    "    for index, row in df.iterrows():\n",
    "        for phrase in splitting_phrases:\n",
    "            split_phrases = row['sentence'].split(phrase)\n",
    "            if len(split_phrases) > 1:\n",
    "                df.loc[index,'left_split'] = split_phrases[0]\n",
    "                df.loc[index,'right_split'] = split_phrases[1]\n",
    "                df.loc[index,'splitting_phrase'] = phrase\n",
    "                break\n",
    "    df.drop_duplicates\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POC of splitting technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# since split_sentences_on_common_phrases is df agnostic, it must be fed only sentences that you want analyzed. Thus, drop duplicates from the feed df that creates the split_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_counted_df, split_phrases = common_phrase_with_count(immigrant_deported,blocking_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>speaker</th>\n",
       "      <th>dupe_phrases</th>\n",
       "      <th>left_split</th>\n",
       "      <th>right_split</th>\n",
       "      <th>splitting_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35207</th>\n",
       "      <td>where kate steinle was murdered by an illegal ...</td>\n",
       "      <td>trump</td>\n",
       "      <td>by an illegal immigrant and deported</td>\n",
       "      <td>where kate steinle was</td>\n",
       "      <td>probably more than five times</td>\n",
       "      <td>murdered by an illegal immigrant and deported</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41382</th>\n",
       "      <td>hillary supports total open borders -- that me...</td>\n",
       "      <td>trump</td>\n",
       "      <td>by an illegal immigrant and deported</td>\n",
       "      <td>hillary supports total open borders -- that me...</td>\n",
       "      <td></td>\n",
       "      <td>murdered by an illegal immigrant and deported</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30570</th>\n",
       "      <td>hillary supports totally open borders there go...</td>\n",
       "      <td>trump</td>\n",
       "      <td>by an illegal immigrant deported at</td>\n",
       "      <td>hillary supports totally open borders there go...</td>\n",
       "      <td>times</td>\n",
       "      <td>murdered by an illegal immigrant deported at l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28085</th>\n",
       "      <td>and strongly supports sanctuary cities like sa...</td>\n",
       "      <td>trump</td>\n",
       "      <td>by an illegal immigrant deported at</td>\n",
       "      <td>and strongly supports sanctuary cities like sa...</td>\n",
       "      <td>times</td>\n",
       "      <td>murdered by an illegal immigrant deported at l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42777</th>\n",
       "      <td>and strongly supports sanctuary cities like sa...</td>\n",
       "      <td>trump</td>\n",
       "      <td>by an illegal immigrant deported at</td>\n",
       "      <td>and strongly supports sanctuary cities like sa...</td>\n",
       "      <td>times</td>\n",
       "      <td>illegal immigrant deported at least five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19613</th>\n",
       "      <td>there goes your country -- and strongly suppor...</td>\n",
       "      <td>trump</td>\n",
       "      <td>by an illegal immigrant deported at</td>\n",
       "      <td>there goes your country -- and strongly suppor...</td>\n",
       "      <td>times</td>\n",
       "      <td>murdered by an illegal immigrant deported at l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38886</th>\n",
       "      <td>and strongly supports sanctuary cities like sa...</td>\n",
       "      <td>trump</td>\n",
       "      <td>by an illegal immigrant who was deported</td>\n",
       "      <td>and strongly supports sanctuary cities like sa...</td>\n",
       "      <td>probably more than five times</td>\n",
       "      <td>murdered by an illegal immigrant who was deported</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34074</th>\n",
       "      <td>hillary clinton supports totally open borders ...</td>\n",
       "      <td>trump</td>\n",
       "      <td>by an illegal immigrant who was deported</td>\n",
       "      <td>hillary clinton supports totally open borders ...</td>\n",
       "      <td>least five times</td>\n",
       "      <td>murdered by an illegal immigrant who was depor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27675</th>\n",
       "      <td>hillary supports totally opened borders there ...</td>\n",
       "      <td>trump</td>\n",
       "      <td>by an illegal immigrant who was deported</td>\n",
       "      <td>hillary supports totally opened borders there ...</td>\n",
       "      <td>least five times</td>\n",
       "      <td>murdered by an illegal immigrant who was depor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26572</th>\n",
       "      <td>where incredible kate steinle was murdered by ...</td>\n",
       "      <td>trump</td>\n",
       "      <td>by an illegal immigrant who had been deported</td>\n",
       "      <td>where incredible kate steinle was</td>\n",
       "      <td>as least five times</td>\n",
       "      <td>murdered by an illegal immigrant who had been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32368</th>\n",
       "      <td>where kate steinle was murdered by an illegal ...</td>\n",
       "      <td>trump</td>\n",
       "      <td>by an illegal immigrant who had been deported</td>\n",
       "      <td>where kate steinle was</td>\n",
       "      <td>probably more than five times</td>\n",
       "      <td>murdered by an illegal immigrant who had been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26965</th>\n",
       "      <td>like san francisco where kate steinle -- incre...</td>\n",
       "      <td>trump</td>\n",
       "      <td>by an illegal immigrant who had been deported</td>\n",
       "      <td>like san francisco where kate steinle -- incre...</td>\n",
       "      <td>five times</td>\n",
       "      <td>murdered by an illegal immigrant who had been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28759</th>\n",
       "      <td>she was gunned down by an illegal immigrant wh...</td>\n",
       "      <td>trump</td>\n",
       "      <td>by an illegal immigrant who had been deported</td>\n",
       "      <td>she was gunned down by an</td>\n",
       "      <td>five times</td>\n",
       "      <td>illegal immigrant who had been deported</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33690</th>\n",
       "      <td>like in san francisco where the incredible kat...</td>\n",
       "      <td>trump</td>\n",
       "      <td>five-time deported illegal immigrant</td>\n",
       "      <td>like in san francisco where the incredible kat...</td>\n",
       "      <td></td>\n",
       "      <td>a five-time deported illegal immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31971</th>\n",
       "      <td>cases like kate steinle murdered in san franci...</td>\n",
       "      <td>trump</td>\n",
       "      <td>five-time deported illegal immigrant</td>\n",
       "      <td>cases like kate steinle murdered in san franci...</td>\n",
       "      <td>who should never ever have been here</td>\n",
       "      <td>a five-time deported illegal immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32373</th>\n",
       "      <td>very tough requirements minimum sentences for ...</td>\n",
       "      <td>trump</td>\n",
       "      <td>for deported illegal immigrant</td>\n",
       "      <td>very tough requirements minimum</td>\n",
       "      <td>s who come back into our country</td>\n",
       "      <td>sentences for deported illegal immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32372</th>\n",
       "      <td>a trump administration will impose tough manda...</td>\n",
       "      <td>trump</td>\n",
       "      <td>for deported illegal immigrant</td>\n",
       "      <td>a trump administration will impose tough manda...</td>\n",
       "      <td>s</td>\n",
       "      <td>sentences for deported illegal immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30923</th>\n",
       "      <td>another reform i'm proposing is the passage of...</td>\n",
       "      <td>trump</td>\n",
       "      <td>previously deported illegal immigrant</td>\n",
       "      <td>another reform i'm proposing is the passage of...</td>\n",
       "      <td></td>\n",
       "      <td>a previously deported illegal immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36455</th>\n",
       "      <td>on mother's day in 2009 carson city-born 17-ye...</td>\n",
       "      <td>trump</td>\n",
       "      <td>previously deported illegal immigrant</td>\n",
       "      <td>on mother's day in 2009 carson city-born 17-ye...</td>\n",
       "      <td>gang member who everybody that knew this gang...</td>\n",
       "      <td>a previously deported illegal immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36453</th>\n",
       "      <td>by an illegal immigrant deported more than fiv...</td>\n",
       "      <td>trump</td>\n",
       "      <td>illegal immigrant deported</td>\n",
       "      <td>by an</td>\n",
       "      <td>more than five times</td>\n",
       "      <td>illegal immigrant deported</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30768</th>\n",
       "      <td>gunned down in the sanctuary city of san franc...</td>\n",
       "      <td>trump</td>\n",
       "      <td>illegal immigrant deported</td>\n",
       "      <td>gunned down in the sanctuary city of san franc...</td>\n",
       "      <td>five previous times</td>\n",
       "      <td>illegal immigrant deported</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25364</th>\n",
       "      <td>where kate steinle was killed by a five time d...</td>\n",
       "      <td>trump</td>\n",
       "      <td>deported illegal immigrant</td>\n",
       "      <td>where kate steinle was killed by a five time</td>\n",
       "      <td></td>\n",
       "      <td>deported illegal immigrant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence speaker  \\\n",
       "35207  where kate steinle was murdered by an illegal ...   trump   \n",
       "41382  hillary supports total open borders -- that me...   trump   \n",
       "30570  hillary supports totally open borders there go...   trump   \n",
       "28085  and strongly supports sanctuary cities like sa...   trump   \n",
       "42777  and strongly supports sanctuary cities like sa...   trump   \n",
       "19613  there goes your country -- and strongly suppor...   trump   \n",
       "38886  and strongly supports sanctuary cities like sa...   trump   \n",
       "34074  hillary clinton supports totally open borders ...   trump   \n",
       "27675  hillary supports totally opened borders there ...   trump   \n",
       "26572  where incredible kate steinle was murdered by ...   trump   \n",
       "32368  where kate steinle was murdered by an illegal ...   trump   \n",
       "26965  like san francisco where kate steinle -- incre...   trump   \n",
       "28759  she was gunned down by an illegal immigrant wh...   trump   \n",
       "33690  like in san francisco where the incredible kat...   trump   \n",
       "31971  cases like kate steinle murdered in san franci...   trump   \n",
       "32373  very tough requirements minimum sentences for ...   trump   \n",
       "32372  a trump administration will impose tough manda...   trump   \n",
       "30923  another reform i'm proposing is the passage of...   trump   \n",
       "36455  on mother's day in 2009 carson city-born 17-ye...   trump   \n",
       "36453  by an illegal immigrant deported more than fiv...   trump   \n",
       "30768  gunned down in the sanctuary city of san franc...   trump   \n",
       "25364  where kate steinle was killed by a five time d...   trump   \n",
       "\n",
       "                                        dupe_phrases  \\\n",
       "35207           by an illegal immigrant and deported   \n",
       "41382           by an illegal immigrant and deported   \n",
       "30570            by an illegal immigrant deported at   \n",
       "28085            by an illegal immigrant deported at   \n",
       "42777            by an illegal immigrant deported at   \n",
       "19613            by an illegal immigrant deported at   \n",
       "38886       by an illegal immigrant who was deported   \n",
       "34074       by an illegal immigrant who was deported   \n",
       "27675       by an illegal immigrant who was deported   \n",
       "26572  by an illegal immigrant who had been deported   \n",
       "32368  by an illegal immigrant who had been deported   \n",
       "26965  by an illegal immigrant who had been deported   \n",
       "28759  by an illegal immigrant who had been deported   \n",
       "33690           five-time deported illegal immigrant   \n",
       "31971           five-time deported illegal immigrant   \n",
       "32373                 for deported illegal immigrant   \n",
       "32372                 for deported illegal immigrant   \n",
       "30923          previously deported illegal immigrant   \n",
       "36455          previously deported illegal immigrant   \n",
       "36453                     illegal immigrant deported   \n",
       "30768                     illegal immigrant deported   \n",
       "25364                     deported illegal immigrant   \n",
       "\n",
       "                                              left_split  \\\n",
       "35207                            where kate steinle was    \n",
       "41382  hillary supports total open borders -- that me...   \n",
       "30570  hillary supports totally open borders there go...   \n",
       "28085  and strongly supports sanctuary cities like sa...   \n",
       "42777  and strongly supports sanctuary cities like sa...   \n",
       "19613  there goes your country -- and strongly suppor...   \n",
       "38886  and strongly supports sanctuary cities like sa...   \n",
       "34074  hillary clinton supports totally open borders ...   \n",
       "27675  hillary supports totally opened borders there ...   \n",
       "26572                 where incredible kate steinle was    \n",
       "32368                            where kate steinle was    \n",
       "26965  like san francisco where kate steinle -- incre...   \n",
       "28759                         she was gunned down by an    \n",
       "33690  like in san francisco where the incredible kat...   \n",
       "31971  cases like kate steinle murdered in san franci...   \n",
       "32373                   very tough requirements minimum    \n",
       "32372  a trump administration will impose tough manda...   \n",
       "30923  another reform i'm proposing is the passage of...   \n",
       "36455  on mother's day in 2009 carson city-born 17-ye...   \n",
       "36453                                             by an    \n",
       "30768  gunned down in the sanctuary city of san franc...   \n",
       "25364      where kate steinle was killed by a five time    \n",
       "\n",
       "                                             right_split  \\\n",
       "35207                      probably more than five times   \n",
       "41382                                                      \n",
       "30570                                              times   \n",
       "28085                                              times   \n",
       "42777                                              times   \n",
       "19613                                              times   \n",
       "38886                      probably more than five times   \n",
       "34074                                   least five times   \n",
       "27675                                   least five times   \n",
       "26572                                as least five times   \n",
       "32368                      probably more than five times   \n",
       "26965                                         five times   \n",
       "28759                                         five times   \n",
       "33690                                                      \n",
       "31971               who should never ever have been here   \n",
       "32373                   s who come back into our country   \n",
       "32372                                                  s   \n",
       "30923                                                      \n",
       "36455   gang member who everybody that knew this gang...   \n",
       "36453                               more than five times   \n",
       "30768                                five previous times   \n",
       "25364                                                      \n",
       "\n",
       "                                        splitting_phrase  \n",
       "35207      murdered by an illegal immigrant and deported  \n",
       "41382      murdered by an illegal immigrant and deported  \n",
       "30570  murdered by an illegal immigrant deported at l...  \n",
       "28085  murdered by an illegal immigrant deported at l...  \n",
       "42777           illegal immigrant deported at least five  \n",
       "19613  murdered by an illegal immigrant deported at l...  \n",
       "38886  murdered by an illegal immigrant who was deported  \n",
       "34074  murdered by an illegal immigrant who was depor...  \n",
       "27675  murdered by an illegal immigrant who was depor...  \n",
       "26572  murdered by an illegal immigrant who had been ...  \n",
       "32368  murdered by an illegal immigrant who had been ...  \n",
       "26965  murdered by an illegal immigrant who had been ...  \n",
       "28759            illegal immigrant who had been deported  \n",
       "33690             a five-time deported illegal immigrant  \n",
       "31971             a five-time deported illegal immigrant  \n",
       "32373           sentences for deported illegal immigrant  \n",
       "32372           sentences for deported illegal immigrant  \n",
       "30923            a previously deported illegal immigrant  \n",
       "36455            a previously deported illegal immigrant  \n",
       "36453                         illegal immigrant deported  \n",
       "30768                         illegal immigrant deported  \n",
       "25364                         deported illegal immigrant  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_sentences_on_common_phrases(immigrant_deported.drop_duplicates(subset=['sentence']),split_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
