{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory: Extract multiple phrases surrounding keywords into a dataset. Then, anonymize the dataset extractions by simply replacing the rex expression with nothing. After that, apply positive-negative sentiment analysis to the phrase to remove any potential connotations invoked by the initial phrasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting phrases rather than just sentential analysis allows for patterns of speech to be extracted from speeches and then compared for frequent useage.\n",
    "\n",
    "## It also helps humans build a phrase anonymizing map to go beyond just single words or a priori judgments about what is in speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict, OrderedDict\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton = pd.read_csv('')\n",
    "trump = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton['month'] = clinton['month'].apply(lambda x: \"{:02d}\".format(x))\n",
    "clinton['date'] = clinton['date'].apply(lambda x: \"{:02d}\".format(x))\n",
    "clinton['year'] = clinton['year'].astype(str)\n",
    "\n",
    "trump['month'] = trump['month'].apply(lambda x: \"{:02d}\".format(x))\n",
    "trump['date'] = trump['date'].apply(lambda x: \"{:02d}\".format(x))\n",
    "trump['year'] = trump['year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rex_phrase_creator(word,before_start,before_stop,after_start,after_stop):\n",
    "    before_rex = []\n",
    "    after_rex = []\n",
    "    before_words = '\\S* '\n",
    "    after_words = ' \\S*'\n",
    "    word_rex = '\\s*{}\\s*'.format(word)\n",
    "    phrases = []\n",
    "    \n",
    "    for i in range(before_start,before_stop):\n",
    "        temp_b = before_words*i\n",
    "        temp_b = temp_b\n",
    "        before_rex.append(('b_{}'.format(i),temp_b))\n",
    "    \n",
    "    for i in range(after_start,after_stop):\n",
    "        temp_a = after_words*i\n",
    "        temp_a = temp_a\n",
    "        after_rex.append(('a_{}'.format(i),temp_a))\n",
    "    \n",
    "    for b_col,b_rex in before_rex:\n",
    "        for a_col,a_rex in after_rex:\n",
    "            total_phrase = '({}{}{})'.format(b_rex,word,a_rex)\n",
    "            column_name = word + '_' + b_col + '_' +  a_col\n",
    "            phrases.append((column_name,total_phrase))\n",
    "\n",
    "    for a_col,a_rex in after_rex:\n",
    "        total_phrase = '({}{})'.format(word,a_rex)\n",
    "        phrases.append((word + '_' + a_col,total_phrase))\n",
    "\n",
    "    for b_col,b_rex in before_rex:\n",
    "        total_phrase = '({}{})'.format(b_rex,word)\n",
    "        phrases.append((word + '_' + b_col,total_phrase))\n",
    "        \n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_for_processing(text_str):\n",
    "    clean_txt = text_str.replace('\\n','').replace('\\\\','').replace(',','')\\\n",
    "                    .replace('!','.').replace('?','.')\\\n",
    "                    .rstrip(' ').lstrip('')\\\n",
    "                    .split('.')\n",
    "    \n",
    "    clean_txt = [x.lstrip(' ').rstrip(' ').lower() for x in clean_txt if x.lstrip(' ').rstrip(' ') != '']\n",
    "    return clean_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_to_dataframe(txt,source_id,speaker):\n",
    "    df = pd.DataFrame(data=txt,columns=['sentence'])\n",
    "    df['source_id'] = source_id\n",
    "    df['speaker'] = speaker\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rex_searches(rex_args):\n",
    "    rexes = []\n",
    "    for search in rex_args:\n",
    "        searches = rex_phrase_creator(*search)\n",
    "        rexes.extend(searches)\n",
    "    return rexes\n",
    "\n",
    "def apply_rex_to_df(df,rexes):\n",
    "    apply_df = df.copy()\n",
    "    for col, rex in rexes:\n",
    "        apply_df[col] = apply_df['sentence'].str.lower().str.extract(rex)\n",
    "    return apply_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building automated functions for duplicate phrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_searches = []\n",
    "def phrase_columns_to_seed_word(df,extraction_seeds):\n",
    "    rexes = []\n",
    "    # Modify to multi-word searches\n",
    "    for seed_word in extraction_seeds:\n",
    "        search_rex = re.compile(r'{}'.format(seed_word))\n",
    "        rexes.append(search_rex)\n",
    "    \n",
    "    seed_word_to_phrase_extractions = []\n",
    "    for seed, rex in zip(extraction_seeds,rexes):\n",
    "        \n",
    "        # Generate a list of the associated extraction columns to the seed words\n",
    "        extracted_columns = [col for col in df.columns if rex.search(col)]\n",
    "        seed_word_to_phrase_extraction.append((seed,extracted_columns))\n",
    "        \n",
    "    return seed_word_to_phrase_extractions\n",
    "\n",
    "def count_of_extracted_phrases_by_seed(df,seed_word_to_phrase_extractions):\n",
    "    local_df = df.copy()\n",
    "    count_and_mention_columns = []\n",
    "    for seed_word, extracted_columns in seed_word_to_phrase_extractions:\n",
    "        count_col_name = seed_word + '_count'\n",
    "        mention_name = seed_word + '_mentioned'\n",
    "        \n",
    "        # Control logic to allow reruns in a notebook environment\n",
    "        if count_col_name in local_df.columns:\n",
    "            local_df = local_df.drop(count_col_name,axis=1)\n",
    "        if mention_name in df.columns:\n",
    "            local_df = local_df.drop(mention_name,axis=1)\n",
    "            \n",
    "        # recombine the tuples of seeds and extraction columns\n",
    "        # with the column associated with the count of how many extracted phrases\n",
    "        # are associated with a sentence\n",
    "        seed_extracted_cols_w_count.append((seed_word,extracted_columns,count_col_name))\n",
    "        \n",
    "        # count_column is a row wise sum of how many phrases the seed pulls out\n",
    "        # with the associated auto-generated phrases\n",
    "        local_df[count_col_name] = local_df[extracted_columns].notnull().sum(axis=1)\n",
    "        \n",
    "        # mention_name is just a boolean to provide simple access to a mask about\n",
    "        # whether there is any successful extraction of the seed from the sentence\n",
    "        local_df[mention_name] = local_df[count_col_name] > 0\n",
    "\n",
    "    return local_df, seed_extracted_cols_w_count\n",
    "\n",
    "# Groups duplicates by the speaker and then returns all the indexes for duplicates for easy comparison\n",
    "def indexes_of_duplicated_phrases(df,phrase_columns,count_col):\n",
    "    local_df = df.copy()\n",
    "    \n",
    "    # defaultdict is used so a column can be easily associated with indicies\n",
    "    # without a host of control logic to construct the key if it doesn't exist\n",
    "    # Also allows for phrase columns to be mixed and matched for with aliases without key collisions\n",
    "    indexes_of_duplicated_phrases = defaultdict(set)\n",
    "    \n",
    "    # pull out the speaker to group-by and the repackaged list of\n",
    "    # the seed based extraction columns\n",
    "    # this is a sub function that requires another loop to pass the seed based columns to it\n",
    "    for speaker,group_df in local_df[['speaker'] + phrase_columns].groupby('speaker'):\n",
    "        \n",
    "        # while a mask is available, this allows future tunability\n",
    "        group_df = group_df[group_df[count_col] > 0]\n",
    "        \n",
    "        # pass over each phrase in phrase based columns to scan for duplicated values\n",
    "        # Currently this requires just one duplication of a phrase, indicating a duplication\n",
    "        # to select that phrase as a potential link between two sentences\n",
    "        #\n",
    "        # this does not return the actual phrases, but the indicies of the phrases\n",
    "        # this allows for cross examination of the indicies to provide further control\n",
    "        # over conditions to use a phrase\n",
    "        for col in phrase_columns:\n",
    "            phrase_duplication_s = local_df[col].dropna()\n",
    "            \n",
    "            # construct the mask for duplications\n",
    "            phrase_duplication_s = phrase_duplication_s.duplicated(keep=False)\n",
    "            \n",
    "            # apply the mask to the series\n",
    "            phrase_duplication_s = phrase_duplication_s.loc[phrase_duplication_s]\n",
    "            indexes_of_duplicated_phrases[col].update(list(phrase_duplication_s.index))\n",
    "            \n",
    "    return indexes_of_duplicated_phrases\n",
    "\n",
    "def reduce_and_associate_sentences_to_phrases(df,indexes_of_duplicated_phrases,phrase_columns):\n",
    "    # from the indicies, pull the sentences with viable phrases out of the initial dataframe\n",
    "    # Returns two different kinds of data. A dictionary of the specific phrase rex to the data\n",
    "    # and a new dataframe of just the sentence, speaker, and matching phrase\n",
    "    local_df = df.copy()\n",
    "    generic_phrase_structure_to_phrases = dict()\n",
    "    all_sentences_to_seeded_phrase = pd.DataFrame(columns=['sentence','speaker','extracting_phrase'])\n",
    "    for col in phrase_columns:\n",
    "        phrase_data = local_df.loc[indexes_of_duplicated_phrases[col],['sentence','speaker',col]]\\\n",
    "                                        .sort_values(by=col)\n",
    "        generic_phrase_structure_to_phrases[col] = phrase_data\n",
    "        combinable_data = phrase_data.copy()\n",
    "        combinable_data.columns = ['sentence','speaker','extracting_phrase']\n",
    "        all_sentences_to_seeded_phrase = all_sentences_to_seeded_phrase.append(combinable_data)\n",
    "        \n",
    "    return generic_phrase_structure_to_phrases, all_sentences_to_seeded_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_phrase_with_count(df,blocking_words):\n",
    "    # simple way to add blocking words to remove words like 'a' or 'by' from calculating\n",
    "    # the length of the actual phrase. So \"by me\" is just \"me\" in counting the complexity of the phrase\n",
    "    phrase_counted_df = pd.DataFrame()\n",
    "    temp_df = df.copy()\n",
    "    for sentence, grouped_df in temp_df.groupby('sentence'):\n",
    "        # reset index because two different phrases will be associated with the same\n",
    "        # sentence index\n",
    "        grouped_df.reset_index(inplace=True)\n",
    "        \n",
    "        # drop duplicates to look only at each individual phrase, not all instances\n",
    "        grouped_df = grouped_df.drop_duplicates(subset=['dupe_phrases'])\n",
    "        first_words = grouped_df['extracting_phrase'].str.extract(r'(^\\w+) ')\n",
    "        first_words = first_words.isin(blocking_words)\n",
    "        grouped_df = grouped_df.loc[~first_words.values.flatten()]\n",
    "        grouped_df['phrase_length'] = grouped_df['extracting_phrase'].str.count(' ')\n",
    "        phrase_length_df = phrase_length_df.append(grouped_df)\n",
    "        \n",
    "    phrase_length_df = phrase_length_df.drop_duplicates(subset=['extracting_phrase'])\n",
    "    \n",
    "    # tunable parameter to select phrase complexity\n",
    "    phrase_length_df = phrase_length_df[phrase_length_df['phrase_length'] > 0]\n",
    "\n",
    "    splitting_phrases = phrase_length_df.sort_values(by='phrase_length',\n",
    "                                                     ascending=False)['extracting_phrase'].values\n",
    "\n",
    "    splitting_phrases = [x.strip() for x in splitting_phrases]\n",
    "    \n",
    "    # return the phrases that no longer contain blocking words with a df that allows \n",
    "    # both human inspectable data for phrases in notebooks and the phrases sorted by length\n",
    "    \n",
    "    return phrase_length_df.sort_values(by='phrase_length',ascending=False), sorted_splitting_phrases\n",
    "\n",
    "\n",
    "def split_sentences_on_common_phrases(df,splitting_phrases):\n",
    "    local_df = df.copy()\n",
    "    local_df = local_df.reset_index()\n",
    "    for index, row in local_df.iterrows():\n",
    "        for phrase in splitting_phrases:\n",
    "            # prevent the phrase from splitting on partial words\n",
    "            if not re.search(r'{}$|{} '.format(phrase,phrase),row['sentence']):\n",
    "                continue\n",
    "            \n",
    "            # loop through the sorted list of phrases to find the longest phrase\n",
    "            # that successfully splits the sentence into two parts\n",
    "            # this is a method to merge all the multiple phrases that successfully\n",
    "            # can be extracted from a sentence into a single maximal phrase that works\n",
    "            # and is cross-referenced against other sentences\n",
    "            split_phrases = row['sentence'].split(phrase)\n",
    "            if len(split_phrases) > 1:\n",
    "                local_df.at[index,'left_split'] = split_phrases[0]\n",
    "                local_df.at[index,'right_split'] = split_phrases[1]\n",
    "                local_df.at[index,'splitting_phrase'] = phrase\n",
    "                break\n",
    "    return local_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_blob_analysis(org_df,column):\n",
    "    df = org_df.copy()\n",
    "    df = df.fillna('')\n",
    "    col_sentiment = column + '_sentiment'\n",
    "    col_subjective = column + '_subjectivity'\n",
    "    df[col_sentiment] = df[column].apply(lambda x: textblob.TextBlob(x).sentiment[0])\n",
    "    df[col_subjective] = df[column].apply(lambda x: textblob.TextBlob(x).sentiment[1])\n",
    "    return df\n",
    "\n",
    "def vader_analysis(org_df,column):\n",
    "    df = org_df.copy()\n",
    "    df = df.fillna('')\n",
    "    pos_col = column + '_vader_pos'\n",
    "    neg_col = column + '_vader_neg'\n",
    "    df[pos_col] = df[column].apply(lambda x: analyser.polarity_scores(x)['pos'])\n",
    "    df[neg_col] = df[column].apply(lambda x: analyser.polarity_scores(x)['neg'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_speech_patterns(org_df):\n",
    "    df = org_df.copy()\n",
    "    for index, row in df.iterrows():\n",
    "        #### Positive parts of speech\n",
    "\n",
    "        if row['beginning_eq_end_pos'] and row['beginning_eq_phrase_pos']:\n",
    "            if row['phrase_eq_end_pos']:\n",
    "                pos_speech_type = 'flat'\n",
    "            elif row['phrase_gt_end_pos']:\n",
    "                pos_speech_type = 'flat_high_to_low'\n",
    "            else:\n",
    "                pos_speech_type = 'flat_low_to_high'\n",
    "\n",
    "        elif not row['beginning_eq_end_pos'] and not row['beginning_eq_phrase_pos'] and row['phrase_eq_end_pos']:\n",
    "            if row['beginning_gt_end_pos']:\n",
    "                pos_speech_type = 'high_to_flat'\n",
    "            else:\n",
    "                pos_speech_type = 'low_to_flat'   \n",
    "\n",
    "        elif not row['beginning_eq_end_pos'] and row['beginning_eq_phrase_pos'] and not row['phrase_eq_end_pos']:\n",
    "            if row['beginning_gt_end_pos']:\n",
    "                pos_speech_type = 'flat_to_low'\n",
    "            else:\n",
    "                pos_speech_type = 'flat_to_high'\n",
    "\n",
    "        elif row['beginning_gt_end_pos'] and row['beginning_gt_phrase_pos']:\n",
    "            if row['phrase_gt_end_pos']:\n",
    "                pos_speech_type = 'monotonic_decline'\n",
    "            else:\n",
    "                pos_speech_type = 'high_low_middle'\n",
    "\n",
    "        elif row['beginning_gt_end_pos'] and not row['beginning_gt_phrase_pos']:\n",
    "            if row['phrase_gt_end_pos']:\n",
    "                pos_speech_type = 'middle_high_low'\n",
    "            else:\n",
    "                pos_speech_type = 'impossible'\n",
    "\n",
    "        elif not row['beginning_gt_end_pos'] and row['beginning_gt_phrase_pos']:\n",
    "            if row['phrase_gt_end_pos']:\n",
    "                pos_speech_type = 'impossible_2'\n",
    "            else:\n",
    "                pos_speech_type = 'middle_low_high'\n",
    "\n",
    "        elif not row['beginning_gt_end_pos'] and not row['beginning_gt_phrase_pos']:\n",
    "            if row['phrase_gt_end_pos']:\n",
    "                pos_speech_type = 'low_high_middle'\n",
    "            else:\n",
    "                pos_speech_type = 'monotonic_increase'\n",
    "\n",
    "        ####### Negative parts of speech  \n",
    "        if row['beginning_eq_end_neg'] and row['beginning_eq_phrase_neg']:\n",
    "            if row['phrase_eq_end_neg']:\n",
    "                neg_speech_type = 'flat'\n",
    "            elif row['phrase_gt_end_neg']:\n",
    "                neg_speech_type = 'flat_high_to_low'\n",
    "            else:\n",
    "                neg_speech_type = 'flat_low_to_high' \n",
    "\n",
    "        elif not row['beginning_eq_end_neg'] and not row['beginning_eq_phrase_neg'] and row['phrase_eq_end_neg']:\n",
    "            if row['beginning_gt_end_neg']:\n",
    "                neg_speech_type = 'high_to_flat'\n",
    "            else:\n",
    "                neg_speech_type = 'low_to_flat'\n",
    "\n",
    "        elif not row['beginning_eq_end_neg'] and row['beginning_eq_phrase_neg'] and not row['phrase_eq_end_neg']:\n",
    "            if row['beginning_gt_end_neg']:\n",
    "                neg_speech_type = 'flat_to_low'\n",
    "            else:\n",
    "                neg_speech_type = 'flat_to_high'\n",
    "\n",
    "        elif not row['beginning_eq_end_neg'] and not row['beginning_eq_phrase_neg']:\n",
    "            if row['beginning_gt_end_neg']:\n",
    "                neg_speech_type = 'high_to_flat'\n",
    "            else:\n",
    "                neg_speech_type = 'low_to_flat' \n",
    "\n",
    "        elif row['beginning_gt_end_neg'] and row['beginning_gt_phrase_neg']:\n",
    "            if row['phrase_gt_end_neg']:\n",
    "                neg_speech_type = 'monotonic_decline'\n",
    "            else:\n",
    "                neg_speech_type = 'high_low_middle'\n",
    "\n",
    "        elif row['beginning_gt_end_neg'] and not row['beginning_gt_phrase_neg']:\n",
    "            if row['phrase_gt_end_neg']:\n",
    "                neg_speech_type = 'middle_high_low'\n",
    "            else:\n",
    "                neg_speech_type = 'impossible'\n",
    "\n",
    "        elif not row['beginning_gt_end_neg'] and row['beginning_gt_phrase_neg']:\n",
    "            if row['phrase_gt_end_neg']:\n",
    "                neg_speech_type = 'impossible_2'\n",
    "            else:\n",
    "                neg_speech_type = 'middle_low_high'\n",
    "\n",
    "        elif not row['beginning_gt_end_neg'] and not row['beginning_gt_phrase_neg']:\n",
    "            if row['phrase_gt_end_neg']:\n",
    "                neg_speech_type = 'low_high_middle'\n",
    "            else:\n",
    "                neg_speech_type = 'monotonic_increase'\n",
    "\n",
    "        df.loc[index,'positive_speech_pattern'] = pos_speech_type\n",
    "        df.loc[index,'negative_speech_pattern'] = neg_speech_type\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_phrase_dictionaries(sentence_df,searches,investigation_cols):\n",
    "    local_df = sentence_df.copy()\n",
    "    search_rexes = create_rex_searches(searches)\n",
    "    local_df = apply_rex_to_df(local_df,search_rexes)\n",
    "    extracted_phrase_cols = phrase_columns_to_seed_word(local_df,investigation_cols)\n",
    "\n",
    "    local_df,seed_phrase_count_cols = count_of_extracted_phrases_by_seed(local_df,extracted_phrase_cols)\n",
    "\n",
    "    duplicated_indexes = dict()\n",
    "    master_cols = dict()\n",
    "    for seed, phrase_cols, count_col in seed_phrase_count_cols:\n",
    "        \n",
    "        # check if the phrase column also has the count_column\n",
    "        # indexes_of_duplicated_phrases selects this column to do its masking\n",
    "        if count_col not in phrase_cols:\n",
    "            phrase_cols.append(count_col)\n",
    "        indexes_of_duplicated_phrases = indexes_of_duplicated_phrases(local_df,phrase_cols,count_col)\n",
    "        duplicated_indexes[seed] = indexes_of_duplicated_phrases\n",
    "        \n",
    "    seed_to_phrase_data_dict = dict()\n",
    "    seed_to_sentence_and_phrases_dict = dict()\n",
    "    for seed, phrase_cols, count_col in seed_phrase_count_cols:\n",
    "        phrase_cols = [col for col in phrase_cols if '_count' not in col]\n",
    "        seed_to_phrase_data,seed_to_sentence_and_phrases = reduce_and_associate_sentences_to_phrases(local_df,\n",
    "                                                                             duplicated_indexes[seed],\n",
    "                                                                             phrase_cols)\n",
    "        seed_to_phrase_data_dict[seed] = seed_to_phrase_data\n",
    "        seed_to_sentence_and_phrases_dict[seed] = seed_to_sentence_and_phrases\n",
    "    \n",
    "    return seed_to_sentence_and_phrases_dict, seed_to_phrase_data_dict, local_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = pd.DataFrame(columns=['sentence','source_id','speaker'])\n",
    "for index, row in clinton.iterrows():\n",
    "    source_id = row['year'] + row['month'] + row['date']\n",
    "    source_txt = row['text']\n",
    "    cleaned_txt = clean_data_for_processing(source_txt)\n",
    "    cleaned_df = split_text_to_dataframe(cleaned_txt,source_id,'clinton')\n",
    "    sentence_df = sentence_df.append(cleaned_df)\n",
    "\n",
    "for index, row in trump.iterrows():\n",
    "    source_id = row['year'] + row['month'] + row['date']\n",
    "    source_txt = row['text']\n",
    "    cleaned_txt = clean_data_for_processing(source_txt)\n",
    "    cleaned_df = split_text_to_dataframe(cleaned_txt,source_id,'trump')\n",
    "    sentence_df = sentence_df.append(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = sentence_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df.columns = ['sentence_location','sentence','source_id','speaker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches = [('god',1,4,1,4),\n",
    "            ('country',3,6,3,6),('our country',3,5,3,5),\n",
    "            ('america',3,6,3,6),('our america',3,5,3,5),\n",
    "            ('illegal',1,5,1,5),('illegal alien',2,5,2,5), \n",
    "            ('illegal immigrant',2,5,2,5),('immigrant',1,5,1,5),\n",
    "            ('democracy',1,4,1,4),('huge',1,3,1,3),('great',1,3,1,3),\n",
    "            ('amazing',1,3,1,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeding_columns = ['immigrant','god','america','democracy','huge','great','amazing','great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_to_sentence_and_phrases_dict, seed_to_phrase_data_dict, master_df = set_up_seed_to_phrase_data_dictionaries(sentence_df,\n",
    "                                                                             searches,\n",
    "                                                                             seeding_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocking_words = ['was','is','an','by']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['immigrant', 'god', 'america', 'democracy', 'huge', 'great', 'amazing'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_to_sentence_and_phrases_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_phrases_dfs = []\n",
    "for seed in seed_to_sentence_and_phrases_dict.keys():\n",
    "    phrase_counted_df, sorted_splitting_phrases = common_phrase_with_count(seed_to_sentence_and_phrases_dict[seed],blocking_words)\n",
    "    split_phrases_df = split_sentences_on_common_phrases(seed_to_sentence_and_phrases_dict[seed]\\\n",
    "                                                   .drop_duplicates(subset=['sentence']),sorted_splitting_phrases)\n",
    "    split_phrases_dfs.append(split_phrases_df)\n",
    "\n",
    "split_phrases_df = pd.concat(split_phrases_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>speaker</th>\n",
       "      <th>dupe_phrases</th>\n",
       "      <th>left_split</th>\n",
       "      <th>right_split</th>\n",
       "      <th>splitting_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>44049</td>\n",
       "      <td>i have great foreign advisers</td>\n",
       "      <td>trump</td>\n",
       "      <td>have great</td>\n",
       "      <td></td>\n",
       "      <td>foreign advisers</td>\n",
       "      <td>i have great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>15384</td>\n",
       "      <td>i mean it was such a great thing</td>\n",
       "      <td>trump</td>\n",
       "      <td>a great thing</td>\n",
       "      <td>i mean it was</td>\n",
       "      <td>thing</td>\n",
       "      <td>such a great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>27880</td>\n",
       "      <td>we are living through the greatest jobs theft ...</td>\n",
       "      <td>trump</td>\n",
       "      <td>the great</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>210</td>\n",
       "      <td>we're going to invest in america again</td>\n",
       "      <td>clinton</td>\n",
       "      <td>to invest in america</td>\n",
       "      <td>we're going</td>\n",
       "      <td>again</td>\n",
       "      <td>to invest in america</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>18738</td>\n",
       "      <td>and by the way that's number one from the huma...</td>\n",
       "      <td>trump</td>\n",
       "      <td>that's great</td>\n",
       "      <td>and by the way that's number one from the huma...</td>\n",
       "      <td></td>\n",
       "      <td>that's great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>36833</td>\n",
       "      <td>we are going to provide school choice to every...</td>\n",
       "      <td>trump</td>\n",
       "      <td>america and we are</td>\n",
       "      <td>we are going to provide school choice to every...</td>\n",
       "      <td>end common core</td>\n",
       "      <td>income child in america and we are going to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>11608</td>\n",
       "      <td>and i saw those great beautiful buildings that...</td>\n",
       "      <td>trump</td>\n",
       "      <td>great beautiful</td>\n",
       "      <td>and i saw those</td>\n",
       "      <td>beautiful buildings that were empty and rotti...</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>26654</td>\n",
       "      <td>this is -- i think you people are amazing</td>\n",
       "      <td>trump</td>\n",
       "      <td>are amazing</td>\n",
       "      <td>this is -- i think you</td>\n",
       "      <td></td>\n",
       "      <td>people are amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>33409</td>\n",
       "      <td>we're doing great ohio and florida we're doing...</td>\n",
       "      <td>trump</td>\n",
       "      <td>doing great</td>\n",
       "      <td></td>\n",
       "      <td>ohio and florida</td>\n",
       "      <td>we're doing great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>45230</td>\n",
       "      <td>one of them there 21 years and 15 years great ...</td>\n",
       "      <td>trump</td>\n",
       "      <td>great people</td>\n",
       "      <td>one of them there 21 years and 15 years</td>\n",
       "      <td>you know it's just a question of time</td>\n",
       "      <td>great people and</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                           sentence  speaker  \\\n",
       "1358  44049                      i have great foreign advisers    trump   \n",
       "252   15384                   i mean it was such a great thing    trump   \n",
       "1578  27880  we are living through the greatest jobs theft ...    trump   \n",
       "689     210             we're going to invest in america again  clinton   \n",
       "1487  18738  and by the way that's number one from the huma...    trump   \n",
       "67    36833  we are going to provide school choice to every...    trump   \n",
       "937   11608  and i saw those great beautiful buildings that...    trump   \n",
       "124   26654          this is -- i think you people are amazing    trump   \n",
       "1337  33409  we're doing great ohio and florida we're doing...    trump   \n",
       "1117  45230  one of them there 21 years and 15 years great ...    trump   \n",
       "\n",
       "              dupe_phrases                                         left_split  \\\n",
       "1358            have great                                                      \n",
       "252          a great thing                                     i mean it was    \n",
       "1578             the great                                                NaN   \n",
       "689   to invest in america                                       we're going    \n",
       "1487          that's great  and by the way that's number one from the huma...   \n",
       "67      america and we are  we are going to provide school choice to every...   \n",
       "937        great beautiful                                   and i saw those    \n",
       "124            are amazing                            this is -- i think you    \n",
       "1337           doing great                                                      \n",
       "1117          great people           one of them there 21 years and 15 years    \n",
       "\n",
       "                                            right_split  \\\n",
       "1358                                   foreign advisers   \n",
       "252                                               thing   \n",
       "1578                                                NaN   \n",
       "689                                               again   \n",
       "1487                                                      \n",
       "67                                      end common core   \n",
       "937    beautiful buildings that were empty and rotti...   \n",
       "124                                                       \n",
       "1337                                  ohio and florida    \n",
       "1117              you know it's just a question of time   \n",
       "\n",
       "                                 splitting_phrase  \n",
       "1358                                 i have great  \n",
       "252                                  such a great  \n",
       "1578                                          NaN  \n",
       "689                          to invest in america  \n",
       "1487                                 that's great  \n",
       "67    income child in america and we are going to  \n",
       "937                                         great  \n",
       "124                            people are amazing  \n",
       "1337                            we're doing great  \n",
       "1117                             great people and  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_phrases_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_phrases_df = text_blob_analysis(split_phrases_df,'left_split')\n",
    "# split_phrases_df = text_blob_analysis(split_phrases_df,'right_split')\n",
    "# split_phrases_df = text_blob_analysis(split_phrases_df,'splitting_phrase')\n",
    "# split_phrases_df = text_blob_analysis(split_phrases_df,'sentence')\n",
    "\n",
    "split_phrases_df = vader_analysis(split_phrases_df,'left_split')\n",
    "split_phrases_df = vader_analysis(split_phrases_df,'right_split')\n",
    "split_phrases_df = vader_analysis(split_phrases_df,'splitting_phrase')\n",
    "split_phrases_df = vader_analysis(split_phrases_df,'sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_comparisons = [('beginning_gt_end_neg',['left_split_vader_neg','right_split_vader_neg']),\n",
    "                    ('beginning_gt_phrase_neg',['left_split_vader_neg','splitting_phrase_vader_neg']),\n",
    "                    ('phrase_gt_end_neg',['splitting_phrase_vader_neg','right_split_vader_neg']),\n",
    "                    ('beginning_gt_end_pos',['left_split_vader_pos','right_split_vader_pos']),\n",
    "                    ('beginning_gt_phrase_pos',['left_split_vader_pos','splitting_phrase_vader_pos']),\n",
    "                    ('phrase_gt_end_pos',['splitting_phrase_vader_pos','right_split_vader_pos'])]\n",
    "\n",
    "eq_cross_comparisons = [('beginning_eq_end_neg',['left_split_vader_neg','right_split_vader_neg']),\n",
    "                    ('beginning_eq_phrase_neg',['left_split_vader_neg','splitting_phrase_vader_neg']),\n",
    "                    ('phrase_eq_end_neg',['splitting_phrase_vader_neg','right_split_vader_neg']),\n",
    "                    ('beginning_eq_end_pos',['left_split_vader_pos','right_split_vader_pos']),\n",
    "                    ('beginning_eq_phrase_pos',['left_split_vader_pos','splitting_phrase_vader_pos']),\n",
    "                    ('phrase_eq_end_pos',['splitting_phrase_vader_pos','right_split_vader_pos'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cross_comp in cross_comparisons:\n",
    "    split_phrases_df[cross_comp[0]] = split_phrases_df[cross_comp[1][0]] > split_phrases_df[cross_comp[1][1]]\n",
    "\n",
    "for cross_comp in eq_cross_comparisons:\n",
    "    split_phrases_df[cross_comp[0]] = split_phrases_df[cross_comp[1][0]] == split_phrases_df[cross_comp[1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_phrases_df = apply_speech_patterns(split_phrases_df)\n",
    "\n",
    "split_phrases_df['full_speech_pattern'] = split_phrases_df['positive_speech_pattern'] + '__' + split_phrases_df['negative_speech_pattern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>speaker</th>\n",
       "      <th>dupe_phrases</th>\n",
       "      <th>left_split</th>\n",
       "      <th>right_split</th>\n",
       "      <th>splitting_phrase</th>\n",
       "      <th>left_split_vader_pos</th>\n",
       "      <th>left_split_vader_neg</th>\n",
       "      <th>right_split_vader_pos</th>\n",
       "      <th>...</th>\n",
       "      <th>phrase_gt_end_pos</th>\n",
       "      <th>beginning_eq_end_neg</th>\n",
       "      <th>beginning_eq_phrase_neg</th>\n",
       "      <th>phrase_eq_end_neg</th>\n",
       "      <th>beginning_eq_end_pos</th>\n",
       "      <th>beginning_eq_phrase_pos</th>\n",
       "      <th>phrase_eq_end_pos</th>\n",
       "      <th>positive_speech_pattern</th>\n",
       "      <th>negative_speech_pattern</th>\n",
       "      <th>full_speech_pattern</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>8914</td>\n",
       "      <td>great guys</td>\n",
       "      <td>trump</td>\n",
       "      <td>great guys</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>great guys</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>low_high_middle</td>\n",
       "      <td>flat</td>\n",
       "      <td>low_high_middle__flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>35544</td>\n",
       "      <td>that is how we will truly make america great a...</td>\n",
       "      <td>trump</td>\n",
       "      <td>america great again</td>\n",
       "      <td>that is how we will truly</td>\n",
       "      <td></td>\n",
       "      <td>make america great again</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>middle_high_low</td>\n",
       "      <td>flat</td>\n",
       "      <td>middle_high_low__flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>15949</td>\n",
       "      <td>we're living through the greatest jobs theft i...</td>\n",
       "      <td>trump</td>\n",
       "      <td>the great</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>flat</td>\n",
       "      <td>flat</td>\n",
       "      <td>flat__flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>35754</td>\n",
       "      <td>we have so many endorsements from such great g...</td>\n",
       "      <td>trump</td>\n",
       "      <td>great great</td>\n",
       "      <td>we have so many endorsements from such</td>\n",
       "      <td>winners</td>\n",
       "      <td>great great</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>low_to_flat</td>\n",
       "      <td>flat</td>\n",
       "      <td>low_to_flat__flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>27613</td>\n",
       "      <td>i want the entire corrupt washington establish...</td>\n",
       "      <td>trump</td>\n",
       "      <td>our great congressman</td>\n",
       "      <td>i want the entire corrupt washington establish...</td>\n",
       "      <td>over here; he's not corrupt; where's our cong...</td>\n",
       "      <td>our great congressman</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>middle_high_low</td>\n",
       "      <td>flat</td>\n",
       "      <td>middle_high_low__flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>27726</td>\n",
       "      <td>we we because this is the great movement there...</td>\n",
       "      <td>trump</td>\n",
       "      <td>great movement</td>\n",
       "      <td>we we because this is</td>\n",
       "      <td>movement there's never been anything like thi...</td>\n",
       "      <td>the great</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>low_high_middle</td>\n",
       "      <td>flat_to_high</td>\n",
       "      <td>low_high_middle__flat_to_high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>3001</td>\n",
       "      <td>in fact i just ran across the story in las veg...</td>\n",
       "      <td>clinton</td>\n",
       "      <td>the great</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>flat</td>\n",
       "      <td>flat</td>\n",
       "      <td>flat__flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>16650</td>\n",
       "      <td>we need -- we lost a great justice justice scalia</td>\n",
       "      <td>trump</td>\n",
       "      <td>great justice</td>\n",
       "      <td>we need -- we</td>\n",
       "      <td>justice justice scalia</td>\n",
       "      <td>lost a great</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.872</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>monotonic_increase</td>\n",
       "      <td>low_high_middle</td>\n",
       "      <td>monotonic_increase__low_high_middle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10762</td>\n",
       "      <td>hillary clinton wants to have completely gover...</td>\n",
       "      <td>trump</td>\n",
       "      <td>freedoms of all america this is what</td>\n",
       "      <td>hillary clinton wants to have completely gover...</td>\n",
       "      <td>that's what she's aiming at</td>\n",
       "      <td>liberties and freedoms of all america this is ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>low_high_middle</td>\n",
       "      <td>flat</td>\n",
       "      <td>low_high_middle__flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>33024</td>\n",
       "      <td>so we have to make great deals before we do an...</td>\n",
       "      <td>trump</td>\n",
       "      <td>make great deals</td>\n",
       "      <td>so we have</td>\n",
       "      <td>before we do anything with cuba or anybody el...</td>\n",
       "      <td>to make great deals</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>low_high_middle</td>\n",
       "      <td>flat</td>\n",
       "      <td>low_high_middle__flat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                           sentence  speaker  \\\n",
       "1030   8914                                         great guys    trump   \n",
       "386   35544  that is how we will truly make america great a...    trump   \n",
       "1549  15949  we're living through the greatest jobs theft i...    trump   \n",
       "1007  35754  we have so many endorsements from such great g...    trump   \n",
       "608   27613  i want the entire corrupt washington establish...    trump   \n",
       "1097  27726  we we because this is the great movement there...    trump   \n",
       "1588   3001  in fact i just ran across the story in las veg...  clinton   \n",
       "1074  16650  we need -- we lost a great justice justice scalia    trump   \n",
       "19    10762  hillary clinton wants to have completely gover...    trump   \n",
       "564   33024  so we have to make great deals before we do an...    trump   \n",
       "\n",
       "                              dupe_phrases  \\\n",
       "1030                            great guys   \n",
       "386                    america great again   \n",
       "1549                             the great   \n",
       "1007                           great great   \n",
       "608                  our great congressman   \n",
       "1097                        great movement   \n",
       "1588                             the great   \n",
       "1074                         great justice   \n",
       "19    freedoms of all america this is what   \n",
       "564                       make great deals   \n",
       "\n",
       "                                             left_split  \\\n",
       "1030                                                      \n",
       "386                          that is how we will truly    \n",
       "1549                                                      \n",
       "1007            we have so many endorsements from such    \n",
       "608   i want the entire corrupt washington establish...   \n",
       "1097                             we we because this is    \n",
       "1588                                                      \n",
       "1074                                     we need -- we    \n",
       "19    hillary clinton wants to have completely gover...   \n",
       "564                                         so we have    \n",
       "\n",
       "                                            right_split  \\\n",
       "1030                                                      \n",
       "386                                                       \n",
       "1549                                                      \n",
       "1007                                            winners   \n",
       "608    over here; he's not corrupt; where's our cong...   \n",
       "1097   movement there's never been anything like thi...   \n",
       "1588                                                      \n",
       "1074                             justice justice scalia   \n",
       "19                          that's what she's aiming at   \n",
       "564    before we do anything with cuba or anybody el...   \n",
       "\n",
       "                                       splitting_phrase  left_split_vader_pos  \\\n",
       "1030                                         great guys                 0.000   \n",
       "386                            make america great again                 0.367   \n",
       "1549                                                                    0.000   \n",
       "1007                                        great great                 0.000   \n",
       "608                               our great congressman                 0.157   \n",
       "1097                                          the great                 0.000   \n",
       "1588                                                                    0.000   \n",
       "1074                                       lost a great                 0.000   \n",
       "19    liberties and freedoms of all america this is ...                 0.000   \n",
       "564                                 to make great deals                 0.000   \n",
       "\n",
       "      left_split_vader_neg  right_split_vader_pos  ...  phrase_gt_end_pos  \\\n",
       "1030                 0.000                  0.000  ...               True   \n",
       "386                  0.000                  0.000  ...               True   \n",
       "1549                 0.000                  0.000  ...              False   \n",
       "1007                 0.000                  1.000  ...              False   \n",
       "608                  0.000                  0.000  ...               True   \n",
       "1097                 0.000                  0.000  ...               True   \n",
       "1588                 0.000                  0.000  ...              False   \n",
       "1074                 0.000                  0.872  ...              False   \n",
       "19                   0.227                  0.000  ...               True   \n",
       "564                  0.000                  0.000  ...               True   \n",
       "\n",
       "      beginning_eq_end_neg  beginning_eq_phrase_neg  phrase_eq_end_neg  \\\n",
       "1030                  True                     True               True   \n",
       "386                   True                     True               True   \n",
       "1549                  True                     True               True   \n",
       "1007                  True                     True               True   \n",
       "608                   True                     True               True   \n",
       "1097                 False                     True              False   \n",
       "1588                  True                     True               True   \n",
       "1074                  True                    False              False   \n",
       "19                   False                    False               True   \n",
       "564                   True                     True               True   \n",
       "\n",
       "      beginning_eq_end_pos  beginning_eq_phrase_pos  phrase_eq_end_pos  \\\n",
       "1030                  True                    False              False   \n",
       "386                  False                    False              False   \n",
       "1549                  True                     True               True   \n",
       "1007                 False                    False               True   \n",
       "608                  False                    False              False   \n",
       "1097                  True                    False              False   \n",
       "1588                  True                     True               True   \n",
       "1074                 False                    False              False   \n",
       "19                    True                    False              False   \n",
       "564                   True                    False              False   \n",
       "\n",
       "      positive_speech_pattern  negative_speech_pattern  \\\n",
       "1030          low_high_middle                     flat   \n",
       "386           middle_high_low                     flat   \n",
       "1549                     flat                     flat   \n",
       "1007              low_to_flat                     flat   \n",
       "608           middle_high_low                     flat   \n",
       "1097          low_high_middle             flat_to_high   \n",
       "1588                     flat                     flat   \n",
       "1074       monotonic_increase          low_high_middle   \n",
       "19            low_high_middle                     flat   \n",
       "564           low_high_middle                     flat   \n",
       "\n",
       "                      full_speech_pattern  \n",
       "1030                low_high_middle__flat  \n",
       "386                 middle_high_low__flat  \n",
       "1549                           flat__flat  \n",
       "1007                    low_to_flat__flat  \n",
       "608                 middle_high_low__flat  \n",
       "1097        low_high_middle__flat_to_high  \n",
       "1588                           flat__flat  \n",
       "1074  monotonic_increase__low_high_middle  \n",
       "19                  low_high_middle__flat  \n",
       "564                 low_high_middle__flat  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_phrases_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_trump = split_phrases_df[split_phrases_df['speaker'] == 'trump']\n",
    "pattern_clinton = split_phrases_df[split_phrases_df['speaker'] == 'clinton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7288801571709234"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_clinton['full_speech_pattern'].value_counts() / pattern_clinton.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7459309249702263"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_trump['full_speech_pattern'].value_counts() / pattern_trump.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "low_high_middle       0.682811\n",
       "middle_high_low       0.187773\n",
       "flat                  0.086542\n",
       "monotonic_increase    0.021437\n",
       "monotonic_decline     0.013894\n",
       "low_to_flat           0.004764\n",
       "high_to_flat          0.001588\n",
       "flat_to_high          0.000794\n",
       "middle_low_high       0.000397\n",
       "Name: positive_speech_pattern, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_trump['positive_speech_pattern'].value_counts() / pattern_trump.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flat               0.865026\n",
       "high_to_flat       0.061532\n",
       "flat_to_high       0.054387\n",
       "low_high_middle    0.010322\n",
       "low_to_flat        0.008734\n",
       "Name: negative_speech_pattern, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_trump['negative_speech_pattern'].value_counts() / pattern_trump.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "low_high_middle       0.664047\n",
       "middle_high_low       0.204322\n",
       "flat                  0.078585\n",
       "monotonic_increase    0.035363\n",
       "monotonic_decline     0.013752\n",
       "flat_to_high          0.001965\n",
       "low_to_flat           0.001965\n",
       "Name: positive_speech_pattern, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_clinton['positive_speech_pattern'].value_counts() / pattern_clinton.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flat               0.846758\n",
       "high_to_flat       0.080550\n",
       "flat_to_high       0.043222\n",
       "low_high_middle    0.019646\n",
       "low_to_flat        0.009823\n",
       "Name: negative_speech_pattern, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_clinton['negative_speech_pattern'].value_counts() / pattern_clinton.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
